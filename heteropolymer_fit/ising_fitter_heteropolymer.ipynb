{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Ising fitter for capped repeat proteins with mutations.\n",
    "\n",
    "Authors:  Doug Barrick, Jacob D. Marold, Kathryn Geiger-Schuller, Tural Aksel, Ekaterina Poliakova-Georgantas, Sean Klein, Kevin Sforza, Mark Peterson\n",
    "\n",
    "This notebook reads data from Aviv data files, converts the data to normalized unfolding transitions, generates partition functions and expressions for fraction folded, and uses these expressions to fit the normalized transitions.  Data and fits are plotted in various ways, and bootstrap analysis is performed.  Correlation plots are generated for pairs of bootstrap parameter values.\n",
    "\n",
    "In this notebook, constructs of NRC type are fitted along with constructs of NXC type, where X repeats are R repeats with point substitutions.  The data analyzed are consensus ankyrin repeets collected by Tural Aksel, along with T4V-substituted consensus ankyrin repeats collected by Kevin Sforza.  The data conversion script takes two csv files, one containing the NRC data and the other containing the T4V data (I don't have the original Aviv .dat files for T4V) and are separated and converted into .npy files for fitting.  This script also generates the constructs.json and melts.json lists.\n",
    "\n",
    "Here I am fitting two separate m values, one for R and one for X repeats.  This fits better than one m-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports, path, and project name\n",
    "\n",
    "Path and project name should be set by the user.  Note that because of the kernel restart below, these must be specified in subsequent scripts, along with any other imports that are needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "import glob     \n",
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import sympy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import lmfit\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "proj_name = 'T4V_NRC_2mi'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data conversion from *two* csv files.\n",
    "\n",
    "In this script, data are read from two csv files using pandas dataframes.  There is a csv file for the T4V data, and a second for the NRC data.  Imported to dataframes and combined into a final dataframe.\n",
    "Outputs are\n",
    "\n",
    "1.  A numpy data file for each melt, contining [denaturant], normalized signal, construct ID, and melt ID.\n",
    "\n",
    "2.  A list of constructs.\n",
    "\n",
    "3.  A list of melts.\n",
    "\n",
    "4.  A combined csv file with all melts (T4V and NRC).\n",
    "\n",
    "For the lists of constructs and melts, there are loops that attempt to put the constructs and melts in order so that they appear in a logical progression from short to long and depending on repeat type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "den_nsig_const_melt = []\n",
    "constructs = []         # List of constructs used to build partition functions in next script\n",
    "melts = []              # List of melts to be used in fitting script\n",
    "\n",
    "t4v_filepath = os.path.join(path, 'T4Vdata_not_normalized.csv')\n",
    "nrc_filepath = os.path.join(path, 'NRC_data_dnmn.csv')\n",
    "\n",
    "T4V_input_df = pd.read_csv(t4v_filepath,names=['denat','signal','construct_melt','dataset'])\n",
    "NRC_input_df = pd.read_csv(nrc_filepath,names=['denat','signal','construct_melt','dataset'])\n",
    "maxT4Vmelt = T4V_input_df['dataset'].max()  # Finds the maximum number of melts in the first df\n",
    "NRC_input_df['dataset'] = NRC_input_df['dataset'] + maxT4Vmelt # Adds the max number to the melt numbers in second df\n",
    "combined_input_df = pd.concat([T4V_input_df, NRC_input_df],names=['denat','signal','construct_melt','dataset'])\n",
    "combined_input_df.to_csv(os.path.join(path, 'T4V_NRC_dnmn.csv'), index=False, header=False)\n",
    "\n",
    "num_melts=combined_input_df['dataset'].max()\n",
    "for melt in np.arange(num_melts)+1:\n",
    "    temp_df = combined_input_df.loc[combined_input_df.dataset == melt] # Pulls out just one melt\n",
    "        \n",
    "    # Normalizing the signal\n",
    "    min = temp_df['signal'].min()\n",
    "    max = temp_df['signal'].max()\n",
    "    series=(temp_df['signal'] - min)/(max - min)\n",
    "    temp_df['signal'] = series  # Overwrites un-normalized signal\n",
    "    temp_df.rename(columns={'signal': 'nsig'}, inplace=True)\n",
    "    temp_list = temp_df.values.tolist()\n",
    "    temp_nparray = np.array(temp_list)\n",
    "    construct_melt = temp_df.iloc[0, 2]\n",
    "    np.save(os.path.join(path, construct_melt), temp_nparray) # Writes an npy file to disk for each melt.\n",
    "    melts.append(construct_melt)\n",
    "\n",
    "''' \n",
    "This loop puts melts in order of type (NRxC, NRx, RxC) and length.  This is useful for the\n",
    "plotting script below, putting the by_melt legends in a sensible order\n",
    "'''\n",
    "NRClist = []\n",
    "NRlist = []\n",
    "RClist = []\n",
    "melts.sort()\n",
    "i = 0\n",
    "for melt in melts:\n",
    "    if melt[0] == 'N':\n",
    "        if melt[-3] == 'C':\n",
    "            NRClist.append(melt)\n",
    "        else:\n",
    "            NRlist.append(melt)\n",
    "    else:\n",
    "        RClist.append(melt)\n",
    "\n",
    "NRClist.sort(key=len)\n",
    "NRlist.sort(key=len)\n",
    "RClist.sort(key=len)\n",
    "melts = NRClist + NRlist + RClist\n",
    "    \n",
    "# Generate a list of just the constructs.  The loop removes duplicates.\n",
    "for melt in melts: \n",
    "    if melt[:-2] not in constructs: \n",
    "        constructs.append(melt[:-2]) \n",
    "         \n",
    "with open(os.path.join(path, f\"{proj_name}_constructs.json\"), 'w') as r:\n",
    "    json.dump(constructs, r)\n",
    "\n",
    "with open(os.path.join(path, f\"{proj_name}_melts.json\"), 'w') as s:\n",
    "    json.dump(melts, s)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a partition function and fraction folded expressions for fitting.\n",
    "\n",
    "Inputs the constructs.json, melts.json, and processed .npy data files from the data processing script above.\n",
    "\n",
    "Generates a dictionary of partition functions using the capped homopolymer 1D-Ising model, and converts these to dictionaries of fraction-folded expressions (**fraction_folded_dict**) for fitting by partial differentiation.  Manipulations are done using the sympy module which allows symbolic math operations.  This is important for partial differentiation, but also for \"simplification\" of the fraction folded exprssions.  This simplification factors common terms, significantly decreasing the time it takes to fit and bootstrap below.  The fraction-folded dictionary is exported in json format.\n",
    "\n",
    "Because the numpy exponential function (np.exp) gets reassigned in this script, and I cannot figure out how to undo this, the kernel must be restarted at the bottom of the script (exit()).  The user will be prompted to accept.\n",
    "\n",
    "Though the path, project name, and most (but not all imports) are redundant with the command above, the kernel restart at the end of this script can create problems, if the script is run more than once.  For this reason I am keeping them associated with this script (and with subsequent scripts--fitting, plotting, etc).\n",
    "\n",
    "Note that on 2020_05_05, I am changing the equation that gives the denaturant dependence of DGi to DGi + mi denat in the three equations for N, R, and C.  This corresponds to a positive m-value (free energies become more positive with denaturant).  Also change initial guess in the fitting cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "print('Generating partition functions and fraction folded expressions.  This may take a minute...')\n",
    "\n",
    "# Parameters for partition function calculation.  Note these are sympy symbols.\n",
    "RT = sp.Symbol('RT')\n",
    "dGN = sp.Symbol('dGN')\n",
    "dGR = sp.Symbol('dGR')\n",
    "dGX = sp.Symbol('dGX')\n",
    "dGC = sp.Symbol('dGC')\n",
    "dGRR = sp.Symbol('dGRR')\n",
    "dGXX = sp.Symbol('dGXX')\n",
    "dGXR = sp.Symbol('dGXR')\n",
    "dGRX = sp.Symbol('dGRX')\n",
    "mR = sp.Symbol('mR')\n",
    "mX = sp.Symbol('mX')\n",
    "denat = sp.Symbol('denat')\n",
    "KN = sp.Symbol('KN')\n",
    "KR = sp.Symbol('KR')\n",
    "KX = sp.Symbol('KX')\n",
    "KC = sp.Symbol('KC')\n",
    "TRR = sp.Symbol('TRR')\n",
    "TXX = sp.Symbol('TXX')\n",
    "TXR = sp.Symbol('TXR')\n",
    "TRX = sp.Symbol('TRX')\n",
    "\n",
    "exp = sp.Function('np.exp')\n",
    "\n",
    "with open(os.path.join(path, f\"{proj_name}_constructs.json\"), 'r') as cons:\n",
    "    constructs = json.load(cons)\n",
    "\n",
    "#define weight matricies and end vectors to be used to calculate partition functions\n",
    "begin = sp.Matrix([[0,1]])\n",
    "woN = sp.Matrix([[KN,1],[KN,1]]) # Leave off coupling term.  No zeroth repeat\n",
    "woR = sp.Matrix([[KR,1],[KR,1]]) # Leave off coupling term.  No zeroth repeat\n",
    "woX = sp.Matrix([[KX,1],[KX,1]]) # Leave off coupling term.  No zeroth repeat\n",
    "\n",
    "wnR = sp.Matrix([[KR*TRR,1],[KR,1]]) # Treat coupling same as an rR interface, as usual\n",
    "wrR = sp.Matrix([[KR*TRR,1],[KR,1]]) # Same as wnR above\n",
    "wxR = sp.Matrix([[KR*TXR,1],[KR,1]])\n",
    "\n",
    "wnX = sp.Matrix([[KX*TRX,1],[KX,1]]) # Different coupling than \n",
    "wrX = sp.Matrix([[KX*TRX,1],[KX,1]]) # Different coupling than wnX above\n",
    "wxX = sp.Matrix([[KX*TXX,1],[KX,1]])\n",
    "\n",
    "wrC = sp.Matrix([[KC*TRR,1],[KC,1]]) # Treat copuling same as an rR interface, as usual\n",
    "wxC = sp.Matrix([[KC*TXR,1],[KC,1]])\n",
    "\n",
    "end = sp.Matrix([[1],[1]])\n",
    "\n",
    "# Build a dictionary of these matrices\n",
    "w_dict = {'oN': woN, \n",
    "         'oR': woR,\n",
    "         'oX': woX,\n",
    "         'nR': wnR,\n",
    "         'rR': wrR,\n",
    "         'xR': wxR,\n",
    "         'nX': wnX,\n",
    "         'rX': wrX,\n",
    "         'xX': wxX,\n",
    "         'rC': wrC,\n",
    "         'xC': wxC}\n",
    "\n",
    "# Build dictionaries of partition functions, partial derivs with respect\n",
    "# to K, and fraction folded.\n",
    "\n",
    "q_dict = {}\n",
    "dqdKN_dict = {}\n",
    "dqdKR_dict = {}\n",
    "dqdKX_dict = {}\n",
    "dqdKC_dict = {}\n",
    "frac_folded_dict = {}\n",
    "\n",
    "# Number of repeats of each type.  Seems like they should be floats, but\n",
    "# I get an error in the matrix multiplication (q_dict) if they are declared to be.\n",
    "\n",
    "\n",
    "for construct in constructs:\n",
    "\n",
    "    # Make partition function dictionary and expressions for fraction folded. \n",
    "    # Note, only one pf is generated per construct, even when there are multiple melts.\n",
    "    \n",
    "    repeat_list = construct.split('_')\n",
    "    repeat_list.insert(0, 'o')\n",
    "    \n",
    "    pairs_list = []\n",
    "    i = 1\n",
    "    while i < len(repeat_list):\n",
    "        pair = repeat_list[i-1].lower() + repeat_list[i] # Need to convert prev rept to lower case\n",
    "        pairs_list.append(pair)\n",
    "        i = i + 1\n",
    "    \n",
    "    q_dict[construct + '_q'] = begin\n",
    "    \n",
    "    for pair in pairs_list:\n",
    "        q_dict[construct + '_q'] = q_dict[construct + '_q'] * w_dict[pair]\n",
    "    \n",
    "    q_dict[construct + '_q']  = q_dict[construct + '_q'] * end\n",
    "    \n",
    "    # Next two lines convert from sp.Matrix to np.array to something else.\n",
    "    # Not sure the logic here, but it works.\n",
    "    q_dict[construct + '_q'] = np.array(q_dict[construct + '_q']) \n",
    "    q_dict[construct + '_q'] = q_dict[construct + '_q'].item(0)\n",
    "        \n",
    "    # Partial derivs wrt KN dictionary.\n",
    "    dqdKN_dict[construct + '_dqdKN'] \\\n",
    "        = sp.diff(q_dict[construct + '_q'], KN)\n",
    "\n",
    "    # Partial derivs wrt KR dictionary.\n",
    "    dqdKR_dict[construct + '_dqdKR'] \\\n",
    "        = sp.diff(q_dict[construct + '_q'], KR)\n",
    "    \n",
    "    # Partial derivs wrt KX dictionary.\n",
    "    dqdKX_dict[construct + '_dqdKX'] \\\n",
    "        = sp.diff(q_dict[construct + '_q'], KX)\n",
    "\n",
    "    # Partial derivs wrt KC dictionary.\n",
    "    dqdKC_dict[construct + '_dqdKC'] \\\n",
    "        = sp.diff(q_dict[construct + '_q'], KC)\n",
    "    \n",
    "    # Fraction folded dictionary.  \n",
    "    frac_folded_dict[construct + '_frac_folded'] \\\n",
    "        = (KN/(q_dict[construct + '_q']) * dqdKN_dict[construct + '_dqdKN'] \\\n",
    "         + KR/(q_dict[construct + '_q']) * dqdKR_dict[construct + '_dqdKR'] \\\n",
    "         + KX/(q_dict[construct + '_q']) * dqdKX_dict[construct + '_dqdKX'] \\\n",
    "         + KC/(q_dict[construct + '_q']) * dqdKC_dict[construct + '_dqdKC']) \\\n",
    "        / (len(pairs_list))\n",
    "\n",
    "# The loop below replaces K's and W's the fraction folded terms in the \n",
    "# dictionary with DGs, ms, and denaturant concentrations.  The simplify line\n",
    "# is really important for making compact expressions for fraction folded.\n",
    "# This simplification greatly speeds up fitting.  The last line\n",
    "# converts from a sympy object to a string, to allow for json dump.\n",
    "\n",
    "for construct in frac_folded_dict:\n",
    "    frac_folded_dict[construct] = frac_folded_dict[construct].subs({\n",
    "    KN:(exp(-((dGN + (mR*denat))/RT))), \n",
    "    KR:(exp(-((dGR + (mR*denat))/RT))), \n",
    "    KX:(exp(-((dGX + (mX*denat))/RT))),\n",
    "    KC:(exp(-((dGC + (mR*denat))/RT))),  \n",
    "    TRR:(exp(-dGRR/RT)), \n",
    "    TXX:(exp(-dGXX/RT)), \n",
    "    TXR:(exp(-dGXR/RT)),\n",
    "    TRX:(exp(-dGRX/RT)) }) \n",
    "    frac_folded_dict[construct] = sp.simplify(frac_folded_dict[construct])\n",
    "    frac_folded_dict[construct] = str(frac_folded_dict[construct])                                \n",
    "\n",
    "with open(os.path.join(path, f'{proj_name}_frac_folded_dict.json'), 'w') as f:\n",
    "    json.dump(frac_folded_dict, f)\n",
    " \n",
    "stop = time.time()\n",
    "runtime = stop - start\n",
    "print('\\nThe elapsed time was ' + str(runtime) + ' sec') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the data with the Ising model\n",
    "\n",
    "Processed data files are imported along with the fraction-folded dictionary and construct and melt lists.  The fit is performed with the lmfit module, which has extra functionality over fitting routines in scipy.  \n",
    "\n",
    "Note that if your initial guesses are poor, the fit may be slowed significantly or the fit may not converge.\n",
    "\n",
    "Fitted thermodynamic parameters are outputted to the screen and are written to a csv file.  Baseline parameters are also written to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFitting the data...\\n\")\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "plt.close()\n",
    "plt.clf\n",
    "\n",
    "RT = 0.001987 * 298.15 #  R in kcal/mol/K, T in Kelvin.\n",
    "\n",
    "#  Dictionary of frac folded eqns from partition function generator script.\n",
    "with open(os.path.join(path, f'{proj_name}_frac_folded_dict.json'), 'r') as ffd:\n",
    "    frac_folded_dict = json.load(ffd)\n",
    "\n",
    "with open(os.path.join(path, f'{proj_name}_constructs.json'), 'r') as construct:\n",
    "    constructs = json.load(construct)\n",
    "\n",
    "with open(os.path.join(path, f'{proj_name}_melts.json'), 'r') as m:\n",
    "    melts = json.load(m)\n",
    "\n",
    "num_melts = len(melts)\n",
    "num_constructs = len(constructs)\n",
    "\n",
    "melt_data_dict = {}\n",
    "for melt in melts:\n",
    "   melt_data_dict[melt] = np.load(os.path.join(path, f'{melt}.npy'), allow_pickle=True)\n",
    "\n",
    "# Compile fraction folded expressions.\n",
    "comp_frac_folded_dict = {}\n",
    "for construct in constructs:\n",
    "    frac_folded_string = frac_folded_dict[construct + '_frac_folded']\n",
    "    comp_frac_folded = compile(frac_folded_string, '{}_comp_ff'.format(construct), 'eval')\n",
    "    comp_frac_folded_dict[construct + '_comp_ff'] = comp_frac_folded\n",
    "\n",
    "# CREATE INITIAL GUESSES\n",
    "# First, thermodynamic parameters.  These are Global.\n",
    "init_guesses = lmfit.Parameters()\n",
    "init_guesses.add('dGN', value = 5)\n",
    "init_guesses.add('dGR', value = 4)\n",
    "init_guesses.add('dGX', value = 5)\n",
    "init_guesses.add('dGC', value = 6)\n",
    "init_guesses.add('dGRR', value = -11)\n",
    "init_guesses.add('dGXX', value = -10)\n",
    "init_guesses.add('dGRX', value = -10)\n",
    "init_guesses.add('dGXR', value = -10)\n",
    "init_guesses.add('mR', value = 0.8)\n",
    "init_guesses.add('mX', value = 0.8)\n",
    "\n",
    "# Next, baseline parameters.  These are local.\n",
    "for melt in melts:\n",
    "    init_guesses.add('af_{}'.format(melt), value=0.02)\n",
    "    init_guesses.add('bf_{}'.format(melt), value=1)\n",
    "    init_guesses.add('au_{}'.format(melt), value=0.0)\n",
    "    init_guesses.add('bu_{}'.format(melt), value=0.0)\n",
    "\n",
    "# Transfers init_guesses to params for fitting, but init_guesses are maintained.\n",
    "params = init_guesses\n",
    "\n",
    "def fitting_function(params, denat, frac_folded, melt):\n",
    "    af = params['af_{}'.format(melt)].value\n",
    "    bf = params['bf_{}'.format(melt)].value\n",
    "    au = params['au_{}'.format(melt)].value\n",
    "    bu = params['bu_{}'.format(melt)].value\n",
    "    dGN = params['dGN'].value\n",
    "    dGR = params['dGR'].value\n",
    "    dGX = params['dGX'].value\n",
    "    dGC = params['dGC'].value\n",
    "    dGRR = params['dGRR'].value\n",
    "    dGXX = params['dGXX'].value\n",
    "    dGRX = params['dGRX'].value\n",
    "    dGXR = params['dGXR'].value\n",
    "    mR = params['mR'].value\n",
    "    mX = params['mX'].value\n",
    "    return ((af * denat) + bf) * frac_folded + (((au * denat) + bu) * (1 - frac_folded))\n",
    "\n",
    "# Objective function creates an array of residuals to be used by lmfit minimize.\n",
    "def objective(params):\n",
    "    resid_dict = {}\n",
    "    dGN = params['dGN'].value  \n",
    "    dGR = params['dGR'].value   \n",
    "    dGC = params['dGC'].value\n",
    "    dGX = params['dGX'].value\n",
    "    dGRR = params['dGRR'].value\n",
    "    dGXX = params['dGXX'].value\n",
    "    dGRX = params['dGRX'].value\n",
    "    dGXR = params['dGXR'].value   \n",
    "    mR = params['mR'].value\n",
    "    mX = params['mX'].value\n",
    "    for melt in melts:   \n",
    "        denat = melt_data_dict[melt][:,0]     # A numpy array of type str\n",
    "        norm_sig = melt_data_dict[melt][:,1]  # A numpy array of type str\n",
    "        denat = denat.astype(float) # A numpy array of type float\n",
    "        norm_sig = norm_sig.astype(float) # A numpy array of type float\n",
    "        string_to_eval = comp_frac_folded_dict[melt[:-2] + '_comp_ff']\n",
    "        frac_folded = eval(string_to_eval)\n",
    "        af = params['af_{}'.format(melt)].value\n",
    "        bf = params['bf_{}'.format(melt)].value\n",
    "        au = params['au_{}'.format(melt)].value\n",
    "        bu = params['bu_{}'.format(melt)].value\n",
    "        resid = norm_sig - fitting_function(params, denat, frac_folded, melt)\n",
    "        resid_dict[melt + '_resid'] = resid\n",
    "    residuals = np.concatenate(list(resid_dict.values()))\n",
    "    return residuals\n",
    "\n",
    "# Fit with lmfit\n",
    "result = lmfit.minimize(objective, init_guesses)\n",
    "fit_resid = result.residual\n",
    "\n",
    "# Print out features of the data, the fit, and optimized param values\n",
    "print(\"There are a total of {} data sets.\".format(num_melts))\n",
    "print(\"There are {} observations.\".format(result.ndata))\n",
    "print(\"There are {} fitted parameters.\".format(result.nvarys))\n",
    "print(\"There are {} degrees of freedom. \\n\".format(result.nfree))\n",
    "print(\"The sum of squared residuals (SSR) is: {0:7.4f}\".format(result.chisqr))\n",
    "print(\"The reduced SSR (SSR/DOF): {0:8.6f} \\n\".format(result.redchi))\n",
    "\n",
    "dGN = result.params['dGN'].value\n",
    "dGR = result.params['dGR'].value\n",
    "dGX = result.params['dGX'].value\n",
    "dGC = result.params['dGC'].value\n",
    "dGRR = result.params['dGRR'].value\n",
    "dGXX = result.params['dGXX'].value\n",
    "dGRX = result.params['dGRX'].value\n",
    "dGXR = result.params['dGXR'].value\n",
    "mR = result.params['mR'].value\n",
    "mX = result.params['mX'].value\n",
    "\n",
    "print('Optimized parameter values:')\n",
    "print('dGN = {0:8.4f}'.format(result.params['dGN'].value))\n",
    "print('dGR = {0:8.4f}'.format(result.params['dGR'].value))\n",
    "print('dGX ={0:8.4f}'.format(result.params['dGX'].value))\n",
    "print('dGC ={0:8.4f}'.format(result.params['dGC'].value))\n",
    "print('dGRR ={0:8.4f}'.format(result.params['dGRR'].value))\n",
    "print('dGXX ={0:8.4f}'.format(result.params['dGXX'].value))\n",
    "print('dGRX ={0:8.4f}'.format(result.params['dGRX'].value))\n",
    "print('dGXR ={0:8.4f}'.format(result.params['dGXR'].value))\n",
    "print('mR ={0:8.4f}'.format(result.params['mR'].value))\n",
    "print('mX ={0:8.4f}'.format(result.params['mX'].value))\n",
    "\n",
    "print(\"\\nWriting best fit parameter and baseline files\")\n",
    "\n",
    "# Compile a list of optimized Ising params and write to file.\n",
    "fitted_ising_params = [[\"dGN\", result.params['dGN'].value], \n",
    "                        [\"dGR\", result.params['dGR'].value], \n",
    "                        [\"dGX\", result.params['dGX'].value],\n",
    "                        [\"dGC\", result.params['dGC'].value],\n",
    "                        [\"dGRR\", result.params['dGRR'].value],\n",
    "                        [\"dGXX\", result.params['dGXX'].value],\n",
    "                        [\"dGRX\", result.params['dGRX'].value],\n",
    "                        [\"dGXR\", result.params['dGXR'].value],\n",
    "                        [\"mR\", result.params['mR'].value],\n",
    "                        [\"mX\", result.params['mX'].value],\n",
    "                        [\"Chi**2\",result.chisqr],\n",
    "                        [\"RedChi\",result.redchi]]\n",
    "\n",
    "with open(os.path.join(path, f'{proj_name}_fitted_Ising_params.csv'), \"w\") as n:\n",
    "    writer = csv.writer(n, delimiter=',')\n",
    "    writer.writerows(fitted_ising_params)\n",
    "n.close()\n",
    "\n",
    "# Compile a list of optimized baseline params and write to file.\n",
    "fitted_base_params = []\n",
    "for melt in melts:\n",
    "    af = result.params['af_%s' % (melt)].value            \n",
    "    bf = result.params['bf_%s' % (melt)].value              \n",
    "    au = result.params['au_%s' % (melt)].value          \n",
    "    bu = result.params['bu_%s' % (melt)].value           \n",
    "    fitted_base_params.append([melt, af, bf, au, bu])            \n",
    "with open(os.path.join(path, f'{proj_name}_fitted_baseline_params.csv'), \"w\") as m:              \n",
    "    writer = csv.writer(m, delimiter=',')\n",
    "    writer.writerows(fitted_base_params) \n",
    "m.close()\n",
    "\n",
    "stop = time.time()\n",
    "runtime = stop - start\n",
    "print('\\nThe elapsed time was ' + str(runtime) + ' sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the results of the fit\n",
    "\n",
    "This cell generates four plots.  Two are \"normalized\" data (the data that were actually fit in the scipt above) and fits.  The other two are fraction-folded data and fits.  One each shows all the constructs, which ideally includes multiple melts of each construct, allowing all fits to be inspected.  The other shows only a single melt for each construct (the first one in the melt list for each), simplifying the plot.\n",
    "\n",
    "The resulting plots are dumped to the screen below the cell, and are saved as png files.\n",
    "\n",
    "Note that this script is meant to be run after the fitting script.  If the fit has not been performed in the current session (or the kernel was restarted after the fit--*not usually the case*), then imports will have to be run, along with data and fitted parameters.  That would be pain, so just re-run the fit again, if you find yourself in this situation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nPlotting results...\\n\")\n",
    "\n",
    "# The function \"baseline_adj\" gives an adjusted y value based on fitted baseline \n",
    "# parameters (fraction folded). \n",
    "def baseline_adj(y, x, params, construct):\n",
    "    af = result.params['af_{}'.format(construct)].value\n",
    "    bf = result.params['bf_{}'.format(construct)].value\n",
    "    au = result.params['au_{}'.format(construct)].value\n",
    "    bu = result.params['bu_{}'.format(construct)].value\n",
    "    return (y-(bu+(au*x)))/((bf+(af*x))-(bu+(au*x)))\n",
    "\n",
    "# Defining global best-fit parameters\n",
    "dGN = result.params['dGN'].value\n",
    "dGR = result.params['dGR'].value\n",
    "dGX = result.params['dGX'].value\n",
    "dGC = result.params['dGC'].value\n",
    "dGRR = result.params['dGRR'].value\n",
    "dGXX = result.params['dGXX'].value\n",
    "dGRX = result.params['dGRX'].value\n",
    "dGXR = result.params['dGXR'].value\n",
    "mR = result.params['mR'].value\n",
    "mX = result.params['mX'].value\n",
    "\n",
    "# The function fit_model used for plotting best-fit lines and for adding  \n",
    "# residuals to best-fit lines in bootstrapping.  Normalized, not frac folded.\n",
    "def fit_model(params, x, melt):\n",
    "    denat = x\n",
    "    af = result.params['af_{}'.format(melt)].value\n",
    "    bf = result.params['bf_{}'.format(melt)].value\n",
    "    au = result.params['au_{}'.format(melt)].value\n",
    "    bu = result.params['bu_{}'.format(melt)].value\n",
    "    frac_folded = eval(comp_frac_folded_dict[melt[:-2] + '_comp_ff']) # :-2 leaves off the _1, _2, etc from melt id.    \n",
    "    return ((af * denat) + bf) * frac_folded + (((au * denat) + bu) * \\\n",
    "            (1 - frac_folded))   \n",
    "\n",
    "# Finding the maximum denaturant value out of all the melts to\n",
    "# set x axis bound\n",
    "denat_maxer = np.zeros(0)\n",
    "for melt in melts:\n",
    "    denat_maxer = np.concatenate((denat_maxer, melt_data_dict[melt][:, 0]))\n",
    "denat_maxer_list = denat_maxer.astype(float)\n",
    "denat_max = np.max(denat_maxer_list)\n",
    "denat_bound = np.around(denat_max, 1) + 0.2\n",
    "\n",
    "# Denaturant values to use when evaluating fits.  Determines how smooth the\n",
    "# fitted curve will be, based on the third value (300) in the argument below.\n",
    "# I might keep using this for fraction_foldeed, but for nomralized baseline\n",
    "# use a local set of points for each melt, so as not to extrapolate the \n",
    "# bselines too far.\n",
    "denat_fit = np.linspace(0, denat_bound, 300)\n",
    "\n",
    "#defining a dictionary using the first melt of each construct (construct_1)\n",
    "#Move this to the plotting part, and why not do this for all constructs?\n",
    "construct1_data_dict = {}\n",
    "for construct in constructs:\n",
    "    construct1_data_dict[construct] = np.load(os.path.join(path, f'{construct}_1.npy'))\n",
    "\n",
    "# The four dictionaries below define lower and upper denaturant limnits to be\n",
    "# used for plotting normalized curves, so crazy-long baseline extrapolations \n",
    "# are not shown.  Do both for melts and construct 1.   These are then used\n",
    "# to create 300-point synthetic baselines in the fifth and sixth dictionaries.\n",
    "melt_lower_denat_dict = {melt: round(np.min(melt_data_dict[melt][:,0].astype(float))) -0.2 for melt in melts}\n",
    "melt_upper_denat_dict = {melt: round(np.max(melt_data_dict[melt][:,0].astype(float))) + 0.2 for melt in melts}\n",
    "\n",
    "construct1_lower_denat_dict = {construct: round(np.min(construct1_data_dict[construct][:,0].astype(float))) - 0.2 for construct in constructs}\n",
    "\n",
    "construct1_upper_denat_dict = {construct: round(np.max(construct1_data_dict[construct][:,0].astype(float))) + 0.2 for construct in constructs}\n",
    "\n",
    "melt_denat_synthetic_dict = {\n",
    "    melt: np.linspace(\n",
    "        melt_lower_denat_dict[melt],\n",
    "        melt_upper_denat_dict[melt],\n",
    "        300\n",
    "    ) \n",
    "    for melt in melts\n",
    "}\n",
    "\n",
    "construct1_denat_synthetic_dict = {\n",
    "    construct: np.linspace(\n",
    "        construct1_lower_denat_dict[construct],\n",
    "        construct1_upper_denat_dict[construct], \n",
    "        300\n",
    "    )\n",
    "    for construct in constructs\n",
    "}\n",
    "                        \n",
    "\n",
    "''' Global Plot Aesthetics'''\n",
    "# Defining how the plots are colored\n",
    "num_melt_colors = num_melts\n",
    "num_construct_colors = num_constructs\n",
    "coloration = plt.get_cmap('hsv')\n",
    "\n",
    "\n",
    "# Dictonary defining title font\n",
    "title_font = {'family': 'arial',\n",
    "        'color': 'black',\n",
    "        'weight': 'normal',\n",
    "        'size': 16}\n",
    "\n",
    "# Dictionary defining label font\n",
    "label_font = {'family': 'arial',\n",
    "        'color': 'black',\n",
    "        'weight': 'normal',\n",
    "        'size': 14}\n",
    "\n",
    "'''First Plot: Fraction Folded by Melt'''\n",
    "#extracting the melt data and creating plot lines for each melt\n",
    "colorset = 0 # counter to control color of curves and points\n",
    "for melt in melts:   \n",
    "    colorset = colorset + 1\n",
    "    denat = melt_data_dict[melt][:,0]     # A numpy array of type str\n",
    "    norm_sig = melt_data_dict[melt][:,1]  # A numpy array of type str\n",
    "    denat = denat.astype(float) # A numpy array of type float\n",
    "    norm_sig = norm_sig.astype(float) # A numpy array of type float\n",
    "    y_adj = baseline_adj(norm_sig, denat, result.params, melt)\n",
    "    y_fit = fit_model(result.params, denat_fit, melt)\n",
    "    y_fit_adj = baseline_adj(y_fit, denat_fit, result.params, melt)\n",
    "    plt.plot(denat, y_adj, 'o', color = coloration(colorset/num_melt_colors),\n",
    "            label = melt[:-2] + ' melt ' + melt[-1])       \n",
    "    plt.plot(denat_fit, y_fit_adj, '-', color = coloration(colorset/num_melt_colors))\n",
    "\n",
    "#set axis limits\n",
    "axes=plt.gca()\n",
    "axes.set_xlim([-0.1, denat_bound])\n",
    "axes.set_ylim([-0.1,1.1])\n",
    "axes.set_aspect(5.5)\n",
    "\n",
    "#lot aesthetics and labels\n",
    "plt.legend(loc = 'center', bbox_to_anchor = (1.25, 0.5), fontsize=8)\n",
    "plt.title('Fraction Folded by Melt', fontdict = title_font)\n",
    "plt.xlabel('Denaturant (Molar)', fontdict = label_font)\n",
    "plt.ylabel('Fraction Folded', fontdict = label_font)\n",
    "\n",
    "#saving plot in individual doc\n",
    "plt.savefig(os.path.join(path, f'{proj_name}_plot_frac_folded_by_melt.png'),\\\n",
    "            dpi = 500, bbox_inches='tight')\n",
    "\n",
    "#show plot in iPython window and then close\n",
    "plt.show()\n",
    "plt.close()\n",
    "plt.clf\n",
    "\n",
    "'''Second Plot: Normalized Signal by Melt'''\n",
    "colorset = 0\n",
    "for melt in melts:   \n",
    "    colorset = colorset + 1\n",
    "    denat = melt_data_dict[melt][:,0]     # A numpy array of type str\n",
    "    norm_sig = melt_data_dict[melt][:,1]  # A numpy array of type str\n",
    "    denat = denat.astype(float) # A numpy array of type float\n",
    "    norm_sig = norm_sig.astype(float) # A numpy array of type float\n",
    "    y_fit = fit_model(result.params, melt_denat_synthetic_dict[melt], melt)\n",
    "    plt.plot(denat, norm_sig, 'o', color=coloration(colorset/num_melt_colors),\n",
    "             label = melt[:-2] + ' melt ' + melt[-1]) \n",
    "    plt.plot(melt_denat_synthetic_dict[melt], y_fit, '-', \\\n",
    "             color=coloration(colorset/num_melt_colors))\n",
    "\n",
    "#set axis limits\n",
    "axes=plt.gca()\n",
    "axes.set_xlim([-0.1, denat_bound])\n",
    "axes.set_ylim([-0.1,1.1])\n",
    "axes.set_aspect(5.5)\n",
    "\n",
    "#plot aesthetics and labels\n",
    "plt.legend(loc = 'center', bbox_to_anchor = (1.25, 0.5), fontsize=8)\n",
    "plt.title('Normalized Signal by Melt', fontdict = title_font)\n",
    "plt.xlabel('Denaturant (Molar)', fontdict = label_font)\n",
    "plt.ylabel('Normalized Signal', fontdict = label_font)\n",
    "\n",
    "#saving plot in individual doc\n",
    "plt.savefig(os.path.join(path, f'{proj_name}_plot_normalized_by_melt.png'),\\\n",
    "            dpi=500, bbox_inches='tight')\n",
    "\n",
    "#show plot in iPython window and then close\n",
    "plt.show()\n",
    "plt.close()\n",
    "plt.clf\n",
    "\n",
    "'''Third Plot: Fraction Folded by Construct'''\n",
    "colorset = 0\n",
    "for construct in constructs:   \n",
    "    colorset = colorset + 1\n",
    "    denat = construct1_data_dict[construct][:,0]  # A numpy array of type str\n",
    "    denat_line = construct1_data_dict[construct][:, 0]   # A numpy array of type str\n",
    "    norm_sig = construct1_data_dict[construct][:, 1]  # A numpy array of type str\n",
    "    denat = denat.astype(float) # A numpy array of type float\n",
    "    denat_line = denat_line.astype(float) # A numpy array of type float\n",
    "    norm_sig = norm_sig.astype(float) # A numpy array of type float\n",
    "    y_adj = baseline_adj(norm_sig, denat_line, result.params, construct + '_1')\n",
    "    y_fit = fit_model(result.params, denat_fit, construct + '_1')\n",
    "    y_fit_adj = baseline_adj(y_fit, denat_fit, result.params, construct + '_1')\n",
    "    plt.plot(denat, y_adj, 'o', \\\n",
    "             color=coloration(colorset/num_construct_colors), label = construct)       \n",
    "    plt.plot(denat_fit, y_fit_adj, '-', \\\n",
    "             color=coloration(colorset/num_construct_colors))\n",
    "\n",
    "#set axis limits\n",
    "axes=plt.gca()\n",
    "axes.set_xlim([-0.1, denat_bound])\n",
    "axes.set_ylim([-0.1,1.1])\n",
    "axes.set_aspect(5.5)\n",
    "\n",
    "#plot aesthetics and labels\n",
    "plt.legend(loc = 'center', bbox_to_anchor = (1.15, 0.5), fontsize=8)\n",
    "plt.title('Fraction Folded by Construct', fontdict = title_font)\n",
    "plt.xlabel('Denaturant (Molar)', fontdict = label_font)\n",
    "plt.ylabel('Fraction Folded', fontdict = label_font)\n",
    "\n",
    "#saving plot in individual doc\n",
    "plt.savefig(os.path.join(path, f'{proj_name}_plot_frac_folded_by_construct.png'),\\\n",
    "            dpi=500, bbox_inches='tight')\n",
    "\n",
    "#show plot in iPython window and then close\n",
    "plt.show()\n",
    "plt.close()\n",
    "plt.clf\n",
    "\n",
    "'''Fourth Plot: Normalized Signal by Construct'''\n",
    "colorset = 0\n",
    "for construct in constructs:   \n",
    "    colorset = colorset + 1\n",
    "    denat = construct1_data_dict[construct][:,0]     # A numpy array of type str\n",
    "    norm_sig = construct1_data_dict[construct][:,1]  # A numpy array of type str\n",
    "    denat = denat.astype(float) # A numpy array of type float\n",
    "    norm_sig = norm_sig.astype(float) # A numpy array of type float\n",
    "    y_fit = fit_model(result.params, construct1_denat_synthetic_dict[construct], \\\n",
    "                      construct + '_1')\n",
    "    plt.plot(denat, norm_sig, 'o', color = coloration(colorset/num_construct_colors),\n",
    "             label = construct) \n",
    "    plt.plot(construct1_denat_synthetic_dict[construct], y_fit, '-', \\\n",
    "             color = coloration(colorset/num_construct_colors))\n",
    "\n",
    "#set axis limits\n",
    "axes=plt.gca()\n",
    "axes.set_xlim([-0.1, denat_bound])\n",
    "axes.set_ylim([-0.1,1.1])\n",
    "axes.set_aspect(5.5)\n",
    "\n",
    "#plot aesthetics and labels   * ``````\n",
    "plt.legend(loc = 'center', bbox_to_anchor = (1.15, 0.5), fontsize=8)\n",
    "plt.title('Normalized Signal by Construct', fontdict = title_font)\n",
    "plt.xlabel('Denaturant (Molar)', fontdict = label_font)\n",
    "plt.ylabel('Normalized Signal', fontdict = label_font)\n",
    "\n",
    "#saving plot in individual doc\n",
    "plt.savefig(os.path.join(path, f'{proj_name}_plot_normalized_by_construct.png'),\\\n",
    "            dpi=500, bbox_inches='tight')\n",
    "\n",
    "#show plot in iPython window and then close\n",
    "plt.show()\n",
    "plt.close()\n",
    "plt.clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap analysis\n",
    "\n",
    "Asks the user to input the number of bootstrap iterations.  Bootstrap parameters are stored in a list of lists (**bs_param_values**). After performing the specified number of iterations, bootstrapped thermodynamic parameters are written to a csv file.\n",
    "\n",
    "Again, bootstrapping is meant to be performed after fitting above.  Otherwise, the data and the fit model will have to be re-imported, and the params list and objective function will need to be generated.  Just run the fit again if needed.\n",
    "\n",
    "In this version, a two minute sleep command is built in every 50 bootstrap iterations to let things cool down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''BootStrap analysis'''\n",
    "# Create list to store bootstrap iterations of values and define column titles\n",
    "bs_param_values = []  \n",
    "bs_param_values.append(['Bootstrap Iter', 'dGN', 'dGR', 'dGX', 'dGC', 'dGRR', \n",
    "                        'dGXX', 'dGRX', 'dGXR', 'mR', 'mX', 'redchi**2','bestchi**2'])   \n",
    "#total number of bootstrap iterations \n",
    "bs_iter_tot = input(\"How many bootstrap iterations? \") \n",
    "\n",
    "#bs_iter_tot = 10  # You would use this if you did not want user input from screen\n",
    "bs_iter_count = 0   # Iteration counter\n",
    "fit_resid_index= len(fit_resid) - 1\n",
    "\n",
    "y_fitted_dict = {}\n",
    "# Dictionary of 'true' normalized y values from fit at each denaturant value. \n",
    "for melt in melts:\n",
    "    denat = melt_data_dict[melt][:,0] # A numpy array of type str\n",
    "    denat = denat.astype(float)       # A numpy array of type float\n",
    "    y_fitted_dict[melt] = np.array(fit_model(result.params, denat, melt))\n",
    "\n",
    "# Arrays to store bs fitted param values\n",
    "dGN_vals = []\n",
    "dGR_vals = []\n",
    "dGX_vals = []\n",
    "dGC_vals = []\n",
    "dGRR_vals = []\n",
    "dGXX_vals = []\n",
    "dGRX_vals = []\n",
    "dGXR_vals = []\n",
    "mR_vals = []\n",
    "mX_vals = []\n",
    "\n",
    "# Add residuals chosen at random (with replacement) to expected\n",
    "# y values. Note-residuals are combined ACROSS melts.\n",
    "for j in range(int(bs_iter_tot)):   \n",
    "    rand_resid_dict={}  # Clears the random data for each bootsterap iteration\n",
    "    bs_iter_count = bs_iter_count + 1\n",
    "    print(\"Bootstrap iteration {0} out of {1}\".format(bs_iter_count, \n",
    "                               bs_iter_tot))\n",
    "    \n",
    "    for melt in melts:\n",
    "        rand_resid =[]\n",
    "        denat = melt_data_dict[melt][:,0] # A numpy array of type str\n",
    "        denat = denat.astype(float)       # A numpy array of type float\n",
    "        \n",
    "        for x in range(len(denat)):  # Creastes a list of random residuals\n",
    "            rand_int = np.random.randint(0, fit_resid_index)\n",
    "            rand_resid.append(fit_resid[rand_int])  \n",
    " \n",
    "        rand_resid_dict[melt] = np.array(rand_resid)\n",
    "        y_bootstrap = y_fitted_dict[melt] + rand_resid_dict[melt]\n",
    "        z_max,z_min = y_bootstrap.max(), y_bootstrap.min()\n",
    "        melt_data_dict[melt][:, 1] = (y_bootstrap - z_min)/(z_max - z_min)\n",
    "    \n",
    "    bs_result = lmfit.minimize(objective, init_guesses)\n",
    "    bs_chisqr = bs_result.chisqr\n",
    "    bs_red_chisqr= bs_result.redchi\n",
    "    \n",
    "    dGN = bs_result.params['dGN'].value\n",
    "    dGR = bs_result.params['dGR'].value\n",
    "    dGX = bs_result.params['dGX'].value\n",
    "    dGC = bs_result.params['dGC'].value\n",
    "    dGRR = bs_result.params['dGRR'].value\n",
    "    dGXX = bs_result.params['dGXX'].value\n",
    "    dGRX = bs_result.params['dGRX'].value\n",
    "    dGXR = bs_result.params['dGXR'].value\n",
    "    mR = bs_result.params['mR'].value\n",
    "    mX = bs_result.params['mX'].value\n",
    "    \n",
    "    # Store each value in a list for plotting and for downstream statistical analysis\n",
    "    dGN_vals.append(dGN)\n",
    "    dGR_vals.append(dGR)\n",
    "    dGX_vals.append(dGX)\n",
    "    dGC_vals.append(dGC)\n",
    "    dGRR_vals.append(dGRR)\n",
    "    dGXX_vals.append(dGXX)\n",
    "    dGRX_vals.append(dGRX)\n",
    "    dGXR_vals.append(dGXR)\n",
    "    mR_vals.append(mR)\n",
    "    mX_vals.append(mX)\n",
    "    \n",
    "    # Append bootstrapped global parameter values for ouput to a file\n",
    "    bs_param_values.append([bs_iter_count, dGN, dGR, dGX, dGC, dGRR, dGXX, dGRX, dGXR, mR, mX, \n",
    "                            bs_red_chisqr,bs_chisqr])\n",
    "        \n",
    "with open(os.path.join(path, f'{proj_name}_bootstrap_params.csv'), \"w\") as n:\n",
    "    writer = csv.writer(n, delimiter = ',')\n",
    "    writer.writerows(bs_param_values)\n",
    "n.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The next cell calculates statistical properties of bootstrap parameters and outputs a file\n",
    "\n",
    "I plan to merge this with the bootstrap cell, but it is much more convenient to code it separately.  \n",
    "\n",
    "The structure that currently holds the bootstrap parameter values (*bs_param_values*) is a list of lists.  So it needs to be converted to a numpy array, and it needs to have only values, not column heads, in order to do numerical calculations.  Pandas would clearly be the right way to go with this, but not today.\n",
    "\n",
    "*path* (for writing out the data frame) is taken from the fitting cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_param_values_fullarray = np.array(bs_param_values)\n",
    "bs_param_values_array = bs_param_values_fullarray[1:,1:-2].astype(np.float) # End at -2 since last two columns\n",
    "                                                                            # are chi square statistics\n",
    "\n",
    "bs_param_names = bs_param_values_fullarray[0][1:-2]\n",
    "\n",
    "statistics = ['mean','median','stdev','2.5% CI','16.6% CI','83.7% CI','97.5% CI']\n",
    "\n",
    "bs_statistics_df = pd.DataFrame(columns = statistics)\n",
    "\n",
    "i = 0\n",
    "for param in bs_param_names:\n",
    "    bs_statistics = []\n",
    "    bs_statistics.append(np.mean(bs_param_values_array[:,i]))\n",
    "    bs_statistics.append(np.median(bs_param_values_array[:,i]))\n",
    "    bs_statistics.append(np.std(bs_param_values_array[:,i]))\n",
    "    bs_statistics.append(np.percentile(bs_param_values_array[:,i],2.5))\n",
    "    bs_statistics.append(np.percentile(bs_param_values_array[:,i],16.7))\n",
    "    bs_statistics.append(np.percentile(bs_param_values_array[:,i],83.3))\n",
    "    bs_statistics.append(np.percentile(bs_param_values_array[:,i],97.5))\n",
    "    bs_statistics_df.loc[param] = bs_statistics\n",
    "    i = i + 1\n",
    "\n",
    "bs_statistics_df.to_csv(os.path.join(path, f'{proj_name}_bootstrap_stats.csv'))\n",
    "\n",
    "corr_coef_matrix = np.corrcoef(bs_param_values_array, rowvar = False)\n",
    "corr_coef_df = pd.DataFrame(corr_coef_matrix, columns = bs_param_names, index = bs_param_names)\n",
    "corr_coef_df.to_csv(os.path.join(path, f'{proj_name}_bootstrap_corr_coefs.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs_statistics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_coef_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_coef_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap histograms and correlation plots\n",
    "\n",
    "Plots are generated for the thermodynamic parameters of interest (currently, baseline parameters are not included, thought this would not be hard to generate).  Histograms are generated for each parameter.  Scatter plots are generated for each pair of parameters (not including self-correlation) and arrayed in a grid along with a linear fit.  Shared axes are used in the grid to minimize white-space resulting from labelling each axis.  Thinking about the output as a matrix, the histograms are on the main diagonal, and the correllation plots are off-diagonal elements populating the upper triangle of the matrix.\n",
    "\n",
    "The plot grid is dumped to the screen below, and is also written as a pdf file.\n",
    "\n",
    "As with the plotting and bootstrapping scripts above, this is meant to be run after the fitting script (and after the bootstrapping script immediately above).  If you have not done that, re-run fit and bootstrap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the names of parameters to be compared to see correlation.\n",
    "corr_params = ['dGN', 'dGR', 'dGX', 'dGC', 'dGRR', 'dGXX', 'dGRX', 'dGXR', 'mR', 'mX']\n",
    "\n",
    "# These are a second set of parameter names that follow in the same order\n",
    "# as in corr_params.  They are formatted using TeX-style names so that Deltas \n",
    "# and subscripts will be plotted.  The would not be good key names for dictionaries\n",
    "corr_param_labels = ['$\\Delta$G$_N$', '$\\Delta$G$_R$', '$\\Delta$G$_X$', '$\\Delta$G$_C$', \n",
    "               '$\\Delta$G$_{RR}$', '$\\Delta$G$_{XX}$', '$\\Delta$G$_{RX}$', '$\\Delta$G$_{XR}$', 'm$_R$', 'm$_X$']\n",
    "\n",
    "num_corr_params = len(corr_params)\n",
    "gridsize = num_corr_params  # Determines the size of the plot grid.\n",
    "\n",
    "# Dictionary of fitted parameter values.\n",
    "corr_params_dict = {'dGN': dGN_vals, 'dGR': dGR_vals, 'dGX': dGX_vals, 'dGC': dGC_vals,\\\n",
    "                    'dGRR': dGRR_vals, 'dGXX': dGXX_vals, 'dGRX': dGRX_vals, 'dGXR': dGXR_vals, \n",
    "                    'mR': mR_vals, 'mX': mX_vals}\n",
    "\n",
    "# PDF that stores a grid of the correlation plots\n",
    "with PdfPages(os.path.join(path, f'{proj_name}_Corr_Plots.pdf')) as pdf:\n",
    "    fig, axs = plt.subplots(ncols=gridsize, nrows=gridsize, figsize=(20, 20))\n",
    "    \n",
    "    # Turns off axes on lower triangle\n",
    "    axs[1, 0].axis('off')\n",
    "    axs[2, 0].axis('off')\n",
    "    axs[2, 1].axis('off')\n",
    "    axs[3, 0].axis('off')\n",
    "    axs[3, 1].axis('off')\n",
    "    axs[3, 2].axis('off')\n",
    "    axs[4, 0].axis('off')\n",
    "    axs[4, 1].axis('off')\n",
    "    axs[4, 2].axis('off')\n",
    "    axs[4, 3].axis('off')\n",
    "    axs[5, 0].axis('off')\n",
    "    axs[5, 1].axis('off')\n",
    "    axs[5, 2].axis('off')\n",
    "    axs[5, 3].axis('off')\n",
    "    axs[5, 4].axis('off')\n",
    "    axs[6, 0].axis('off')\n",
    "    axs[6, 1].axis('off')\n",
    "    axs[6, 2].axis('off')\n",
    "    axs[6, 3].axis('off')\n",
    "    axs[6, 4].axis('off')\n",
    "    axs[6, 5].axis('off')\n",
    "    axs[7, 0].axis('off')\n",
    "    axs[7, 1].axis('off')\n",
    "    axs[7, 2].axis('off')\n",
    "    axs[7, 3].axis('off')\n",
    "    axs[7, 4].axis('off')\n",
    "    axs[7, 5].axis('off')\n",
    "    axs[7, 6].axis('off')\n",
    "    axs[8, 0].axis('off')\n",
    "    axs[8, 1].axis('off')\n",
    "    axs[8, 2].axis('off')\n",
    "    axs[8, 3].axis('off')\n",
    "    axs[8, 4].axis('off')\n",
    "    axs[8, 5].axis('off')\n",
    "    axs[8, 6].axis('off')\n",
    "    axs[8, 7].axis('off')\n",
    "    axs[9, 0].axis('off')\n",
    "    axs[9, 1].axis('off')\n",
    "    axs[9, 2].axis('off')\n",
    "    axs[9, 3].axis('off')\n",
    "    axs[9, 4].axis('off')\n",
    "    axs[9, 5].axis('off')\n",
    "    axs[9, 6].axis('off')\n",
    "    axs[9, 7].axis('off')\n",
    "    axs[9, 8].axis('off')\n",
    "\n",
    "    # Defines the position of the y paramater from the array of params\n",
    "    hist_param_counter = 0\n",
    "    while hist_param_counter < num_corr_params:\n",
    "        hist_param_label = corr_param_labels[hist_param_counter]\n",
    "        hist_param = corr_params[hist_param_counter]\n",
    "        # Start fixing labels here\n",
    "        #plt.xticks(fontsize=8)\n",
    "        #axs[hist_param_counter, hist_param_counter].tick_params(fontsize=8)\n",
    "        #axs[hist_param_counter, hist_param_counter].yticks(fontsize=8)\n",
    "        axs[hist_param_counter, hist_param_counter].hist(corr_params_dict[hist_param])\n",
    "        axs[hist_param_counter, hist_param_counter].set_xlabel(hist_param_label,\n",
    "                   fontsize=14, labelpad = 5)\n",
    "        hist_param_counter = hist_param_counter + 1  \n",
    "    \n",
    "    # This part generates the correlation plots\n",
    "    y_param_counter = 0\n",
    "    while y_param_counter < num_corr_params - 1:\n",
    "        # Pulls the parameter name for the y-axis label (with TeX formatting)\n",
    "        yparam_label = corr_param_labels[y_param_counter]\n",
    "        # Pulls the parameter name to be plotted on the y-axis\n",
    "        yparam = corr_params[y_param_counter]\n",
    "        \n",
    "        # Defines the position of the x paramater from the array of params.\n",
    "        # The + 1 offest avoids correlating a parameter with itself.\n",
    "        x_param_counter = y_param_counter + 1\n",
    "        \n",
    "        while (x_param_counter < num_corr_params):\n",
    "            #pulls the parameter name for the x-axis label (with TeX formatting)\n",
    "            xparam_label = corr_param_labels[x_param_counter]\n",
    "            # Pulls the parameter name to be plotted on the x-axis\n",
    "            xparam = corr_params[x_param_counter]\n",
    "            \n",
    "            x_vals= corr_params_dict[xparam]\n",
    "            y_vals = corr_params_dict[yparam]\n",
    "            \n",
    "            #plt.xticks(fontsize=8)\n",
    "            #plt.yticks(fontsize=8)\n",
    "            #plotting scatters with axes.  +1 shifts a plot to the right from main diagonal\n",
    "            axs[y_param_counter, x_param_counter].plot(x_vals, y_vals, '.')\n",
    "            \n",
    "            # The if statement below turns off numbers on axes if not the right column and\n",
    "            # not the main diagonal.\n",
    "            if x_param_counter < num_corr_params - 1:\n",
    "                axs[y_param_counter, x_param_counter].set_xticklabels([])\n",
    "                axs[y_param_counter, x_param_counter].set_yticklabels([])\n",
    "                           \n",
    "            if y_param_counter == 0:  # Puts labels above axes on top row\n",
    "                axs[y_param_counter, x_param_counter].xaxis.set_label_position('top')\n",
    "                axs[y_param_counter, x_param_counter].set_xlabel(xparam_label,\n",
    "                   labelpad = 10, fontsize=14)\n",
    "                axs[y_param_counter, x_param_counter].xaxis.tick_top()\n",
    "                if x_param_counter < num_corr_params - 1:  # Avoids eliminating y-scale from upper right corner\n",
    "                    axs[y_param_counter, x_param_counter].set_yticklabels([])\n",
    "           \n",
    "            if x_param_counter == num_corr_params - 1:  #  Puts labels right of right column\n",
    "                axs[y_param_counter, x_param_counter].yaxis.set_label_position('right')\n",
    "                axs[y_param_counter, x_param_counter].set_ylabel(yparam_label, \n",
    "                   rotation = 0, labelpad = 30, fontsize=14)\n",
    "                axs[y_param_counter, x_param_counter].set_xticklabels([])\n",
    "                axs[y_param_counter, x_param_counter].yaxis.tick_right()\n",
    "               \n",
    "            # Determin correlation coefficient and display under subplot title\n",
    "            # Note, there is no code that displays this value at the moment.\n",
    "            #corr_coef = np.around(np.corrcoef(x_vals, y_vals), 3)\n",
    "            \n",
    "            #min and max values of the x param\n",
    "            x_min = np.min(x_vals)\n",
    "            x_max = np.max(x_vals)\n",
    "            \n",
    "            #fitting a straight line to the correlation scatterplot\n",
    "            fit_array = np.polyfit(x_vals, y_vals, 1)\n",
    "            fit_deg1_coef = fit_array[0]\n",
    "            fit_deg0_coef = fit_array[1]      \n",
    "            fit_x_vals = np.linspace(x_min, x_max, 10)\n",
    "            fit_y_vals = fit_deg1_coef*fit_x_vals + fit_deg0_coef\n",
    "            \n",
    "            #plotting correlation line fits\n",
    "            axs[y_param_counter, x_param_counter].plot(fit_x_vals, \n",
    "               fit_y_vals)\n",
    "            plt.subplots_adjust(wspace=0, hspace=0)\n",
    "            \n",
    "            x_param_counter = x_param_counter + 1            \n",
    "        y_param_counter = y_param_counter + 1\n",
    "    \n",
    "    pdf.savefig(bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
